{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 141\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = []\n",
    "\n",
    "data_type = \"dev\"\n",
    "with open(f\"{data_type}.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "print(\"Number of examples:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tag = set([\n",
    "    \"PER\",   # player\n",
    "    \"CLU\",   # club\n",
    "    \"TME\",   # time\n",
    "    \"NUM\",   # number \n",
    "])\n",
    "\n",
    "relation_tag = set([\n",
    "    \"COMP\",  # (CLU, CLU) compete with\n",
    "    \"DEFE\",  # (CLU, CLU) defeat / win over\n",
    "    # \"SCON\",  # (CLU, NUM) score\n",
    "    \"SCOP\",  # (CLU, PER) score player\n",
    "    \"SCOT\",  # (PER, TME) score time\n",
    "    \"CARP\",  # (CLU, PER) card player,\n",
    "    \"CART\",  # (PER, TIME) card time\n",
    "    \"SUBP\",  # (PER, PER) substitute players,\n",
    "    \"SUBT\",  # (PER, TIME) substitute time\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "segmentor = VnCoreNLP(\"../../../VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity format:\n",
    "\n",
    "```\n",
    "<entity id> <ner_tag> <start_pos> <end_pos> <text>\n",
    "```\n",
    "\n",
    "Ví dụ:\n",
    "```\n",
    "T3-7\tGPE 522 531\tcommunity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity():\n",
    "    global_id = 1\n",
    "    def __init__(self, ner_type, start_pos, end_pos, text):\n",
    "        self.id = f\"T-{Entity.global_id}\"\n",
    "        self.ner_type = ner_type\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.text = text\n",
    "        Entity.global_id += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.id}\\t{self.ner_type} {self.start_pos} {self.end_pos}\\t{self.text}\"\n",
    "        \n",
    "class Relation():\n",
    "    global_id = 1\n",
    "    def __init__(self, relation_type, *args):\n",
    "        self.id = f\"R-{Relation.global_id}\"\n",
    "        self.relation_type = relation_type\n",
    "        self.args = args\n",
    "        Relation.global_id += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        res = f\"{self.id}\\t{self.relation_type}\"\n",
    "        for args_id, entity in enumerate(self.args):\n",
    "            res += f\" Arg{args_id + 1}:{entity.id}\"\n",
    "        return res\n",
    "    \n",
    "def segmentize(text):\n",
    "    text = segmentor.tokenize(text.strip())\n",
    "    return \" \".join(\" \".join(x) for x in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_paragraph(data):\n",
    "    event_texts = {0: \"\"}\n",
    "    event_paddings = {0: 0}\n",
    "    event_ids = []\n",
    "    e_id = None\n",
    "    content = \"\"\n",
    "    \n",
    "    for html in data[\"html_annotation\"]:\n",
    "        for e in BeautifulSoup(html):\n",
    "            try:\n",
    "                e_id = int(e[\"event_id\"])\n",
    "                event_ids.append(e_id)\n",
    "                event_texts[e_id] = \"\"\n",
    "            except: pass\n",
    "            if e_id is None:\n",
    "                continue\n",
    "            event_texts[e_id] += segmentize(e.text) + \" \"\n",
    "    event_ids.sort()\n",
    "    \n",
    "    for i, event in enumerate(event_ids):\n",
    "        cur_event_ids = event_ids[i]\n",
    "        pre_event_ids = event_ids[i - 1] if i != 0 else 0\n",
    "        event_paddings[cur_event_ids] = event_paddings[pre_event_ids] + len(event_texts[pre_event_ids])\n",
    "        content += event_texts[cur_event_ids]\n",
    "    \n",
    "    event_texts[\"0\"] = content\n",
    "    title = segmentize(data[\"original_doc\"][\"_source\"][\"description\"])\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"event_texts\": event_texts,\n",
    "        \"event_paddings\": event_paddings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(team1, team2):\n",
    "    team1 = team1.strip()\n",
    "    team2 = team2.strip()\n",
    "    return team1 in team2 or team2 in team1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_token(text, start, passage):\n",
    "    end = start + len(text)\n",
    "    while end < len(passage) and passage[end] != \" \":\n",
    "        end += 1\n",
    "    while start > 0 and passage[start - 1] != \" \":\n",
    "        start -= 1\n",
    "    text = passage[start : end]\n",
    "    return start, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity_relation(data, event_texts, event_paddings):\n",
    "    entities, relations = [], []\n",
    "    entities_id = set()\n",
    "    def add_entity(entity):\n",
    "        if entity.start_pos == entity.end_pos:\n",
    "            return\n",
    "        entities_id.add(entity.id)\n",
    "        entities.append(entity)\n",
    "    \n",
    "    def add_relation(relation):\n",
    "        for entity in relation.args:\n",
    "            if entity.id not in entities_id:\n",
    "                return\n",
    "        relations.append(relation)\n",
    "    \n",
    "    def parse_entity_other(ner_type, text, ref_event_ids):\n",
    "        text = segmentize(text)\n",
    "        for eid in ref_event_ids.split(\",\"):\n",
    "            pos = event_texts[int(eid)].lower().find(text.lower())\n",
    "            if pos != -1:\n",
    "                pos, text = full_text_token(text, pos, event_texts[int(eid)])\n",
    "                start_pos = pos + event_paddings[int(eid)]\n",
    "                end_pos = start_pos + len(text)\n",
    "                return Entity(ner_type, start_pos, end_pos, text)\n",
    "            \n",
    "        edited_texts = []\n",
    "        if ner_type == \"PER\" and len(text.split()) != 1:\n",
    "            edited_texts = text.split()\n",
    "        elif ner_type == \"TME\" and \"'\" in text:\n",
    "            text = text.replace(\"'\", \"\")\n",
    "            edited_texts = [\"phút thứ \" + text, \"phút \" + text] \n",
    "        for text in edited_texts:\n",
    "            edited_ner = parse_entity_other(ner_type, text, ref_event_ids)\n",
    "            if edited_ner.start_pos != edited_ner.end_pos:\n",
    "                return edited_ner\n",
    "            \n",
    "        return Entity(ner_type, -1, -1, text)\n",
    "    \n",
    "    def parse_entity_score(ner_type, score1, score2, ref_event_ids):\n",
    "        edited_scores = [score1 + \"-\" + score2, score1 + \" - \" + score2, score2 + \"-\" + score1, score2 + \" - \" + score1]\n",
    "        for eid in ref_event_ids.split(\",\"):\n",
    "            for score in edited_scores:\n",
    "                score = segmentize(score)\n",
    "                pos = event_texts[int(eid)].find(score)\n",
    "                if pos != -1:\n",
    "                    pos, score = full_text_token(score, pos, event_texts[int(eid)])\n",
    "                    start_pos = event_paddings[int(eid)] + pos\n",
    "                    end_pos = start_pos + len(score)\n",
    "                    return Entity(ner_type, start_pos, end_pos, score)\n",
    "        return Entity(ner_type, -1, -1, score1 + \"-\" + score2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    id = data[\"train_id\"]\n",
    "    summary = data[\"match_summary\"]\n",
    "    team = summary[\"players\"]\n",
    "    score_board = summary[\"score_board\"]\n",
    "    score_list = summary[\"score_list\"]\n",
    "    card_list = summary[\"card_list\"]\n",
    "    subst_list = summary[\"substitution_list\"]\n",
    "    \n",
    "    # Yield teams' name\n",
    "    team1_ner = parse_entity_other(\"CLU\", team[\"team1\"], team[\"ref_event_ids\"])\n",
    "    team2_ner = parse_entity_other(\"CLU\", team[\"team2\"], team[\"ref_event_ids\"])\n",
    "    add_entity(team1_ner)\n",
    "    add_entity(team2_ner)\n",
    "    add_relation(Relation(\"COMP\", team1_ner, team2_ner))\n",
    "    \n",
    "    # Yield scores\n",
    "    score_ner = parse_entity_score(\"SCO\", score_board[\"score1\"], score_board[\"score2\"], score_board[\"ref_event_ids\"])\n",
    "    # score_ner = parse_entity_score(\"SCO\", score_board[\"score1\"], score_board[\"score2\"], \"0\")\n",
    "    add_entity(score_ner)\n",
    "    if int(score_board[\"score1\"]) > int(score_board[\"score2\"]):\n",
    "        add_relation(Relation(\"DEFE\", team1_ner, team2_ner))\n",
    "    elif int(score_board[\"score1\"]) < int(score_board[\"score2\"]):\n",
    "        add_relation(Relation(\"DEFE\", team2_ner, team1_ner))\n",
    "    else:\n",
    "        add_relation(Relation(\"DRAW\", team1_ner, team2_ner))\n",
    "    \n",
    "    # Yield time\n",
    "    for info in score_list:\n",
    "        player = parse_entity_other(\"PER\", info[\"player_name\"], info[\"ref_event_ids\"])\n",
    "        time = parse_entity_other(\"TME\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "        team = team1_ner if info[\"team\"] == team1_ner.text else team2_ner\n",
    "        add_entity(player)\n",
    "        add_entity(time)\n",
    "        add_relation(Relation(\"SCOP\", team, player))\n",
    "        add_relation(Relation(\"SCOT\", player, time))\n",
    "        \n",
    "        \n",
    "    for info in card_list:\n",
    "        player = parse_entity_other(\"PER\", info[\"player_name\"], info[\"ref_event_ids\"])\n",
    "        time = parse_entity_other(\"TME\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "        team = team1_ner if is_match(info[\"team\"], team1_ner.text) else team2_ner\n",
    "        add_entity(player)\n",
    "        add_entity(time)\n",
    "        add_relation(Relation(\"CARP\", team, player))\n",
    "        add_relation(Relation(\"CART\", player, time))\n",
    "        \n",
    "    for info in subst_list:\n",
    "        player_in = parse_entity_other(\"PER\", info[\"player_in\"], info[\"ref_event_ids\"])\n",
    "        player_out = parse_entity_other(\"PER\", info[\"player_out\"], info[\"ref_event_ids\"])\n",
    "        add_entity(player_in)\n",
    "        add_entity(player_out)\n",
    "        add_entity(time)\n",
    "        add_relation(Relation(\"SUBP\", player_out, player_in))\n",
    "        if \"time\" in info:\n",
    "            time = parse_entity_other(\"TME\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "            add_entity(time)\n",
    "            add_relation(Relation(\"SUBT\", player_in, time))\n",
    "        \n",
    "    return entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-25\tCLU 113 128\tAtletico_Madrid\n",
      "T-26\tCLU 203 208\tBetis\n",
      "T-27\tSCO 43 46\t1-0\n",
      "T-28\tPER 1790 1796\tCorrea\n",
      "T-29\tTME 1650 1652\t74\n",
      "\n",
      "R-17\tCOMP Arg1:T-25 Arg2:T-26\n",
      "R-18\tDEFE Arg1:T-25 Arg2:T-26\n",
      "R-19\tSCOP Arg1:T-26 Arg2:T-28\n",
      "R-20\tSCOT Arg1:T-28 Arg2:T-29\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 1\n",
    "paragraph = parse_paragraph(data[sample_idx])\n",
    "content, event_texts, event_paddings = paragraph[\"content\"], paragraph[\"event_texts\"], paragraph[\"event_paddings\"]\n",
    "entities, relations = parse_entity_relation(data[sample_idx], event_texts, event_paddings)\n",
    "\n",
    "print(*entities, sep=\"\\n\")\n",
    "print()\n",
    "print(*relations, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:18<00:00,  7.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if data_type == \"train\":\n",
    "    data[346][\"match_summary\"][\"card_list\"] = []\n",
    "    data[375][\"match_summary\"][\"score_list\"][1][\"ref_event_ids\"] = \"3\"\n",
    "\n",
    "for x in tqdm(data):\n",
    "    file_id = x[\"train_id\"]\n",
    "    para = parse_paragraph(x)\n",
    "    entities, relations = parse_entity_relation(x, para[\"event_texts\"], para[\"event_paddings\"])\n",
    "    \n",
    "    with open(f\"./{data_type}/{file_id}.txt\", \"w\") as f:\n",
    "        # f.write(para[\"title\"] + \"\\n\")\n",
    "        f.write(para[\"content\"])\n",
    "        \n",
    "    with open(f\"./{data_type}/{file_id}.ann\", \"w\") as f:\n",
    "        for entity in entities:\n",
    "            f.write(str(entity) + \"\\n\")\n",
    "        for relation in relations:\n",
    "            f.write(str(relation) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sài_Gòn FC là đội nhỉnh hơn với nhiều cơ_hội nguy_hiểm được tạo ra . Tuy_nhiên các cầu_thủ lại không_thể_nào tận_dụng được cơ_hội và bất_ngờ thua trước khi Thành Nguyện của Tân_Hiệp Hưng dứt_điểm chính_xác sau tình_huống phản_công nhanh . Ngày_mai , Tân_Hiệp Hưng sẽ đụng_độ nhà đương_kim vô_địch - Thái_Sơn_Nam . Sau bàn thua , các cầu_thủ Sài_Gòn FC đẩy cao đội_hình và liên_tiếp dồn_ép đối_phương . Và những nỗ_lực không biết mệt_mỏi của họ đã được đền_đáp khi Hoàng_Minh đánh gót tinh_tế gỡ hoà 1-1 . Đây cũng là kết_quả cuối_cùng của trận đấu . Sau trận đấu , HLV Nguyễn_Bảo_Trung của Sài_Gòn FC tỏ ra tiếc_nuối khi các cầu_thủ không tận_dụng được cơ_hội và để thua trước . Ông nói : “ Bên đội Sài_Gòn có rất nhiều cơ_hội nhưng các cầu_thủ không tận_dụng được , rồi thua trước nên cuối_cùng phải cố_gắng lội ngược ” . Trong khi đó HLV Phạm_Minh_Giang của Tân_Hiệp Hưng cho rằng thế_trận của trận đấu là năm – năm , cơ_hội chia đều cho cả hai đội và đội nào tận_dụng cơ_hội tốt hơn sẽ giành chiến_thắng và kết_quả hoà là hợp_lí . Về trận đấu với đương_kim vô_địch Thái_Sơn_Nam ngày_mai , HLV Phạm_Minh_Giang cho biết sẽ cố hết_sức để giành kết_quả tốt , ông cho biết : “ Theo tôi ngày_mai đụng_độ Thái_Sơn_Nam là ứng_cử_viên số 1 của chức vô_địch nên trận đấu sẽ rất khó_khăn nhưng chúng_tôi sẽ cố_gắng hết_sức ” . Như_vậy , ngày thi_đấu thứ nhất đã kết_thúc với kết_quả cụ_thể : Sanna_Tourist_Khánh_Hoà thua Hoàng_Thư Đà_Nẵng với tỷ_số 1 – 3 , Hải_Phương_Nam_Phú Nhuận thua 1-4 Sanatech_Khánh_Hoà , Thái_Sơn_Bắc hoà Cao_Bằng với tỷ_số 2 – 2 , Thái_Sơn_Nam thắng Sanna_Khánh_Hoà 2 – 1 và Tân_Hiệp Hưng hoà 1-1 Sài_Gòn FC . Ngày_mai ( 9/4 ) , giải Futsal vô_địch quốc_gia HDBank 2017 sẽ tiếp_tục vòng hai vào lúc 9h sáng với trận đấu giữa Sannatech_Khánh_Hoà gặp Thái_Sơn_Bắc . / . Hà_Khánh / VOV-TP HCM '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para[\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
