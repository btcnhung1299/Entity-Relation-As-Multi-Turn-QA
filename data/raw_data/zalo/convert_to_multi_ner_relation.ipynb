{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 141\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "data = []\n",
    "\n",
    "data_type = \"dev\"\n",
    "with open(f\"{data_type}.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "print(\"Number of examples:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "segmentor = VnCoreNLP(\"../../../../VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity format:\n",
    "\n",
    "```\n",
    "<entity id> <ner_tag> <start_pos> <end_pos> <text>\n",
    "```\n",
    "\n",
    "Ví dụ:\n",
    "```\n",
    "T3-7\tGPE 522 531\tcommunity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity():\n",
    "    global_id = 1\n",
    "    def __init__(self, ner_type, start_pos, end_pos, text):\n",
    "        self.id = f\"T-{Entity.global_id}\"\n",
    "        self.ner_type = ner_type\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.text = text\n",
    "        Entity.global_id += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.id}\\t{self.ner_type} {self.start_pos} {self.end_pos}\\t{self.text}\"\n",
    "        \n",
    "class Relation():\n",
    "    global_id = 1\n",
    "    def __init__(self, relation_type, *args):\n",
    "        self.id = f\"R-{Relation.global_id}\"\n",
    "        self.relation_type = relation_type\n",
    "        self.args = args\n",
    "        Relation.global_id += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        res = f\"{self.id}\\t{self.relation_type}\"\n",
    "        for args_id, entity in enumerate(self.args):\n",
    "            res += f\" Arg{args_id + 1}:{entity.id}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def segmentize(text):\n",
    "    text = segmentor.tokenize(text.strip())\n",
    "    return \" \".join(\" \".join(x) for x in text)\n",
    "\n",
    "def parse_paragraph(data):\n",
    "    event_texts = {0: \"\"}\n",
    "    event_paddings = {0: 0}\n",
    "    event_ids = []\n",
    "    e_id = None\n",
    "    content = \"\"\n",
    "    \n",
    "    for html in data[\"html_annotation\"]:\n",
    "        for e in BeautifulSoup(html, \"html.parser\"):\n",
    "            try:\n",
    "                e_id = int(e[\"event_id\"])\n",
    "                event_ids.append(e_id)\n",
    "                event_texts[e_id] = \"\"\n",
    "            except: pass\n",
    "            if e_id is None:\n",
    "                continue\n",
    "            event_texts[e_id] += segmentize(e.text.replace(\"-\", \" - \")) + \" \"\n",
    "    event_ids.sort()\n",
    "    \n",
    "    for i, event in enumerate(event_ids):\n",
    "        cur_event_ids = event_ids[i]\n",
    "        pre_event_ids = event_ids[i - 1] if i != 0 else 0\n",
    "        event_paddings[cur_event_ids] = event_paddings[pre_event_ids] + len(event_texts[pre_event_ids])\n",
    "        content += event_texts[cur_event_ids]\n",
    "    \n",
    "    event_texts[\"0\"] = content\n",
    "    title = segmentize(data[\"original_doc\"][\"_source\"][\"description\"])\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"event_texts\": event_texts,\n",
    "        \"event_paddings\": event_paddings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(team1, team2):\n",
    "    team1 = team1.strip()\n",
    "    team2 = team2.strip()\n",
    "    return team1 in team2 or team2 in team1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_token(text, start, passage):\n",
    "    end = start + len(text)\n",
    "    while end < len(passage) and passage[end] != \" \":\n",
    "        end += 1\n",
    "    while start > 0 and passage[start - 1] != \" \":\n",
    "        start -= 1\n",
    "    text = passage[start : end]\n",
    "    return start, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_entities_other(ner_type, text, ref_event_ids):\n",
    "    def parse_entity_other(text, eid):\n",
    "        if text == \"\":\n",
    "            return Entity(ner_type, -1, -1, text)\n",
    "        \n",
    "        text = segmentize(text)\n",
    "        pos = event_texts[eid].lower().find(text.lower())\n",
    "        if pos != -1:\n",
    "            pos, text = full_text_token(text, pos, event_texts[eid])\n",
    "            start_pos = pos + event_paddings[eid]\n",
    "            end_pos = start_pos + len(text)\n",
    "            return Entity(ner_type, start_pos, end_pos, text)\n",
    "\n",
    "        edited_texts = []\n",
    "        if ner_type == \"PER\" and len(text.split()) != 1:\n",
    "            edited_texts = text.split()\n",
    "        elif ner_type == \"TME\" and \"'\" in text:\n",
    "            text = text.replace(\"'\", \"\")\n",
    "            edited_texts = [\"phút thứ \" + text, \"phút \" + text] \n",
    "        for text in edited_texts:\n",
    "            edited_ner = parse_entity_other(text, eid)\n",
    "            if edited_ner.start_pos != edited_ner.end_pos:\n",
    "                return edited_ner\n",
    "\n",
    "        return Entity(ner_type, -1, -1, text)\n",
    "\n",
    "    res = []\n",
    "    for eid in ref_event_ids.split(\",\"):\n",
    "        entity = parse_entity_other(text, int(eid))\n",
    "        if entity.start_pos == entity.end_pos or not entity.text:\n",
    "            continue\n",
    "        res.append(entity)\n",
    "    return res\n",
    "\n",
    "\n",
    "def _parse_entities_score(ner_type, score1, score2, ref_event_ids):\n",
    "    def parse_entity_single_score(eid, score, start, end):\n",
    "        pos = event_texts[eid][start : end].find(score)\n",
    "        if pos != -1:\n",
    "            pos, score = full_text_token(score, start + pos, event_texts[eid])\n",
    "            start_pos = event_paddings[eid] + pos\n",
    "            end_pos = start_pos + len(score)\n",
    "            \n",
    "            return Entity(ner_type, start_pos, end_pos, score)\n",
    "        return Entity(ner_type, -1, -1, \"\")\n",
    "    \n",
    "    def parse_entity_couple_score(eid, score1, score2):\n",
    "        edited_scores = [score1 + \" - \" + score2, score2 + \" - \" + score1]      \n",
    "        for score in edited_scores:\n",
    "            score = segmentize(score)\n",
    "            score_ner = parse_entity_single_score(eid, score, 0, -1)\n",
    "            if score_ner.start_pos != score_ner.end_pos:\n",
    "                return score_ner\n",
    "        return Entity(ner_type, -1, -1, \"\")\n",
    "\n",
    "    \n",
    "    score1_entities, score2_entities = [], []\n",
    "    for eid in ref_event_ids.split(\",\"):\n",
    "        eid = int(eid)\n",
    "        entity = parse_entity_couple_score(eid, score1, score2)\n",
    "        if entity.start_pos == entity.end_pos:\n",
    "            score1_entity = parse_entity_single_score(eid, score1, 0, -1)\n",
    "            score2_entity = parse_entity_single_score(eid, score2, 0, -1)\n",
    "        else:\n",
    "            entity.start_pos -= event_paddings[eid]\n",
    "            entity.end_pos -= event_paddings[eid]\n",
    "            score1_entity = parse_entity_single_score(eid, score1, entity.start_pos, entity.end_pos)\n",
    "            score2_entity = parse_entity_single_score(eid, score2, entity.start_pos, entity.end_pos)\n",
    "           \n",
    "        if score1_entity.start_pos != score1_entity.end_pos:\n",
    "            score1_entities.append(score1_entity)\n",
    "        if score2_entity.start_pos != score2_entity.end_pos:\n",
    "            score2_entities.append(score2_entity)\n",
    "        \n",
    "    return score1_entities, score2_entities\n",
    "\n",
    "\n",
    "def _parse_relations(rel_type, args):\n",
    "    res = []\n",
    "    for entities in itertools.product(*args):\n",
    "        res.append(Relation(rel_type, *entities))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity_relation(data, event_texts, event_paddings):\n",
    "    entities, relations = [], []\n",
    "    id = data[\"train_id\"]\n",
    "    summary = data[\"match_summary\"]\n",
    "    team = summary[\"players\"]\n",
    "    score_board = summary[\"score_board\"]\n",
    "    score_list = summary[\"score_list\"]\n",
    "    card_list = summary[\"card_list\"]\n",
    "    subst_list = summary[\"substitution_list\"]\n",
    "    \n",
    "    # Yield teams' name\n",
    "    team1_entities = _parse_entities_other(\"CLU\", team[\"team1\"], team[\"ref_event_ids\"])\n",
    "    team2_entities = _parse_entities_other(\"CLU\", team[\"team2\"], team[\"ref_event_ids\"])\n",
    "    entities.extend(team1_entities + team2_entities)\n",
    "    relations.extend(_parse_relations(\"COMP\", [team1_entities, team2_entities]))\n",
    "    relations.extend(_parse_relations(\"COMP\", [team2_entities, team1_entities]))\n",
    "    \n",
    "    # Yield scores\n",
    "    score1_entities, score2_entities = _parse_entities_score(\"SCO\", score_board[\"score1\"], score_board[\"score2\"], score_board[\"ref_event_ids\"])\n",
    "    entities.extend(score1_entities + score2_entities)\n",
    "    relations.extend(_parse_relations(\"SCOC\", [team1_entities, score1_entities]))\n",
    "    relations.extend(_parse_relations(\"SCOC\", [team2_entities, score2_entities]))\n",
    "\n",
    "    # Yield list\n",
    "    for info in score_list:\n",
    "        player_entities = _parse_entities_other(\"PSC\", info[\"player_name\"], info[\"ref_event_ids\"])\n",
    "        time_entities = _parse_entities_other(\"TSC\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "        team_entities = _parse_entities_other(\"CLU\", info[\"team\"], info[\"ref_event_ids\"])\n",
    "        entities.extend(player_entities + time_entities + team_entities)\n",
    "        relations.extend(_parse_relations(\"SCOP\", [player_entities, team_entities]))\n",
    "        relations.extend(_parse_relations(\"SCOT\", [player_entities, time_entities]))       \n",
    "        \n",
    "    for info in card_list:\n",
    "        player_entities = _parse_entities_other(\"PCA\", info[\"player_name\"], info[\"ref_event_ids\"])\n",
    "        time_entities = _parse_entities_other(\"TCA\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "        team_entities = _parse_entities_other(\"CLU\", info[\"team\"], info[\"ref_event_ids\"])\n",
    "        entities.extend(player_entities + time_entities + team_entities)\n",
    "        relations.extend(_parse_relations(\"CARP\", [player_entities, team_entities]))\n",
    "        relations.extend(_parse_relations(\"CART\", [player_entities, time_entities]))   \n",
    "        \n",
    "    for info in subst_list:\n",
    "        playerin_entities = _parse_entities_other(\"PSI\", info[\"player_in\"], info[\"ref_event_ids\"])\n",
    "        playerout_entities = _parse_entities_other(\"PSO\", info[\"player_out\"], info[\"ref_event_ids\"])\n",
    "        time_entities = [] if \"time\" not in info else _parse_entities_other(\"TSI\", info[\"time\"], info[\"ref_event_ids\"])\n",
    "        entities.extend(playerin_entities + playerout_entities + time_entities)\n",
    "        relations.extend(_parse_relations(\"SUBP\", [playerin_entities, playerout_entities]))\n",
    "        relations.extend(_parse_relations(\"SUBT\", [playerin_entities, time_entities]))\n",
    "        \n",
    "    return entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 120\n",
    "paragraph = parse_paragraph(data[sample_idx])\n",
    "content, event_texts, event_paddings = paragraph[\"content\"], paragraph[\"event_texts\"], paragraph[\"event_paddings\"]\n",
    "entities, relations = parse_entity_relation(data[sample_idx], event_texts, event_paddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-1\tCLU 61 72\tBournemouth\n",
      "T-2\tCLU 21 28\tArsenal\n",
      "T-4\tSCO 46 47\t3\n",
      "T-7\tSCO 3032 3033\t3\n",
      "T-5\tSCO 46 47\t3\n",
      "T-8\tSCO 3032 3033\t3\n",
      "T-9\tPSC 1051 1066\tCharlie_Daniels\n",
      "T-10\tTSC 963 965\t19\n",
      "T-11\tCLU 789 800\tBournemouth\n",
      "T-12\tPSC 4165 4176\tRyan_Fraser\n",
      "T-13\tTSC 4241 4243\t58\n",
      "T-14\tCLU 4222 4233\tBournemouth\n",
      "T-15\tPSC 2565 2579\tAlexis_Sanchez\n",
      "T-16\tTSC 2503 2505\t70\n",
      "T-17\tCLU 2640 2647\tArsenal\n",
      "T-18\tPSC 2782 2793\tLucas_Perez\n",
      "T-19\tTSC 2654 2664\t5 phút sau\n",
      "T-20\tCLU 2701 2708\tArsenal\n",
      "T-22\tPSC 1343 1356\tCallum_Wilson\n",
      "T-23\tTSC 1111 1121\t2 phút sau\n",
      "T-25\tCLU 1243 1254\tBournemouth\n",
      "T-26\tCLU 1414 1425\tBournemouth\n",
      "T-27\tPSC 4368 4382\tOlivier_Giroud\n",
      "T-28\tTSC 4354 4365\tphút bù giờ\n",
      "T-29\tCLU 4275 4282\tArsenal\n",
      "T-30\tPCA 2932 2945\tSimon_Francis\n",
      "T-31\tTCA 2920 2922\t82\n",
      "T-32\tCLU 2950 2961\tBournemouth\n",
      "T-33\tPSI 2123 2134\tLucas_Perez\n",
      "T-34\tPSO 2148 2153\tIwobi\n",
      "\n",
      "R-1\tCOMP Arg1:T-1 Arg2:T-2\n",
      "R-2\tCOMP Arg1:T-2 Arg2:T-1\n",
      "R-3\tSCOC Arg1:T-1 Arg2:T-4\n",
      "R-4\tSCOC Arg1:T-1 Arg2:T-7\n",
      "R-5\tSCOC Arg1:T-2 Arg2:T-5\n",
      "R-6\tSCOC Arg1:T-2 Arg2:T-8\n",
      "R-7\tSCOP Arg1:T-9 Arg2:T-11\n",
      "R-8\tSCOT Arg1:T-9 Arg2:T-10\n",
      "R-9\tSCOP Arg1:T-12 Arg2:T-14\n",
      "R-10\tSCOT Arg1:T-12 Arg2:T-13\n",
      "R-11\tSCOP Arg1:T-15 Arg2:T-17\n",
      "R-12\tSCOT Arg1:T-15 Arg2:T-16\n",
      "R-13\tSCOP Arg1:T-18 Arg2:T-20\n",
      "R-14\tSCOT Arg1:T-18 Arg2:T-19\n",
      "R-15\tSCOP Arg1:T-22 Arg2:T-25\n",
      "R-16\tSCOP Arg1:T-22 Arg2:T-26\n",
      "R-17\tSCOT Arg1:T-22 Arg2:T-23\n",
      "R-18\tSCOP Arg1:T-27 Arg2:T-29\n",
      "R-19\tSCOT Arg1:T-27 Arg2:T-28\n",
      "R-20\tCARP Arg1:T-30 Arg2:T-32\n",
      "R-21\tCART Arg1:T-30 Arg2:T-31\n",
      "R-22\tSUBP Arg1:T-33 Arg2:T-34\n"
     ]
    }
   ],
   "source": [
    "print(*entities, sep=\"\\n\")\n",
    "print()\n",
    "print(*relations, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:17<00:00,  8.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "if data_type == \"train\":\n",
    "    data[83][\"match_summary\"][\"score_list\"][1][\"ref_event_ids\"] = \"4\"\n",
    "    data[361][\"match_summary\"][\"score_list\"][0][\"ref_event_ids\"] = \"4\"\n",
    "    data[346][\"match_summary\"][\"card_list\"] = []\n",
    "    data[375][\"match_summary\"][\"score_list\"][1][\"ref_event_ids\"] = \"3\"\n",
    "    \n",
    "for x in tqdm(data):\n",
    "    file_id = x[\"train_id\"]\n",
    "    paragraph = parse_paragraph(x)\n",
    "    content, event_texts, event_paddings = paragraph[\"content\"], paragraph[\"event_texts\"], paragraph[\"event_paddings\"]\n",
    "    entities, relations = parse_entity_relation(x, event_texts, event_paddings)\n",
    "    \n",
    "    with open(f\"./{data_type}/{file_id}.txt\", \"w\") as f:\n",
    "        f.write(paragraph[\"content\"])\n",
    "    \n",
    "    with open(f\"./{data_type}/{file_id}.ann\", \"w\") as f:\n",
    "        for entity in entities:\n",
    "            f.write(str(entity) + \"\\n\")\n",
    "        for relation in relations:\n",
    "            f.write(str(relation) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 211\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "data = []\n",
    "\n",
    "data_type = \"test\"\n",
    "with open(f\"{data_type}.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "print(\"Number of examples:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211/211 [00:11<00:00, 18.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for x in tqdm(data):\n",
    "    file_id = x[\"test_id\"]\n",
    "    content = \" \".join(segmentize(body[\"text\"].replace(\"-\", \" - \")) for body in x[\"original_doc\"][\"_source\"][\"body\"])\n",
    "    \n",
    "    with open(f\"./{data_type}/{file_id}.txt\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    with open(f\"./{data_type}/{file_id}.ann\", \"w\") as f:\n",
    "        for entity in entities:\n",
    "            f.write(\"\")\n",
    "        for relation in relations:\n",
    "            f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
